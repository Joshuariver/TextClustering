# Web Crawling 코드 기본


rm(list=ls())

library(rvest)
library(dplyr)

basic_url <- 'http://news.donga.com/search?query=bigkini&more=1&range=3&p='


urls <- NULL

for(x in 0:5){
  urls[x+1] <- paste0(basic_url, as.character(x*15+1))
}


links <- NULL

for(url in urls){
  html <- read_html(url)
  links <- c(links, html %>% html_nodes('.searchCont') %>% html_nodes('a') %>% html_attr('href') %>% unique())
}

links <- links[-grep("pdf", links)]

txts <- NULL

for(link in links){
  html <- read_html(link)
  txts <- c(txts, html %>% html_nodes('.article_txt') %>% html_text())
}

write.csv(txts, "text.csv")  #한글은 읽고 난 뒤에 워드 등으로 다시 인코딩 할 것.

